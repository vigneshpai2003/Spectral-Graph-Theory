\documentclass{math}

\usepackage{bm}
\usepackage{hyperref}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\prob}{Pr}

\title{Spectral Graph Theory}
\author{Vignesh M Pai}
\date{}


\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

\subsection{Eigenvalues and Optimization}

Let $M$ be a $n$ dimensional symmetric matrix.

\begin{definition}[Rayleigh Quotient]
    The Rayleigh quotient for a matrix $M$ is defined as
    \begin{align*}
        R(\vec{x}, M) := \frac{\vec{x}^T M \vec{x}}{\vec{x}^T \vec{x}}
    \end{align*}
    the matrix is ommitted if obvious from context.
\end{definition}

\begin{theorem}
    Let
    \begin{align*}
        \vec{x} \in \arg \max_{\vec{x} \in \mathbb{R}^n - \set{0}} R(\vec{x})
    \end{align*}
    Such an $\vec{x}$ exists and is an eigenvector of $M$ with the maximum eigenvalue $\mu_1$.
    We can write a similar statement for the minimum eigenvalue by minimizing $R$.
\end{theorem}

\begin{theorem}[Spectral Theorem for Symmetric Matrices]
    There exist numbers $\mu_1, ..., \mu_n$ and orthonormal vectors $\vec{\psi}_1, ..., \vec{\psi}_n$ such that $M \vec{\psi}_i = \mu_i \vec{\psi}_i$
    iff for $1 \leq i \leq n$
    \begin{align*}
        \vec{\psi}_i \in \arg \max_{\substack{\norm{\vec{x}} = 1 \\ \vec{x}^T \vec{\psi}_j = 0,\ j < i}} R(\vec{x})
    \end{align*}
    or equivalently
    \begin{align*}
        \vec{\psi}_i \in \arg \min_{\substack{\norm{\vec{x}} = 1 \\ \vec{x}^T \vec{\psi}_j = 0,\ j > i}} R(\vec{x})
    \end{align*}
\end{theorem}

\begin{theorem}[Courant-Fischer Theorem]
    Let $M$ have eigenvalues $\mu_1 \geq \mu_2 \geq ... \geq \mu_n$, then
    \begin{align*}
        \mu_k = \max_{\substack{S \subset \mathbb{R}^n \\ \dim(S) = k}} \min_{\substack{\vec{x} \in S \\ \vec{x} \neq 0}} R(\vec{x}) = \min_{\substack{T \subset \mathbb{R}^n \\ \dim(T) = n - k + 1}} \max_{\substack{\vec{x} \in T \\ \vec{x} \neq 0}} R(\vec{x})
    \end{align*}
    where $S, T$ are subspaces of $\mathbb{R}^n$.
\end{theorem}

\begin{theorem}[Cauchy's Interlacing Theorem]
    Let $A$ be a symmetric real matrix of dimension $n$.
    Let $B$ be obtained by deleting the same row and column of $A$ ($N$ is a principal submatrix of dimension $n - 1$).
    Let $\alpha_1 \geq ... \geq \alpha_n$ be the eigenvalues of $A$ and $\beta_1 \geq ... \geq \beta_{n-1}$ be the eigenvalues of $B$.
    Then for $1 \leq k \leq n - 1$
    \begin{align*}
        \alpha_k \geq \beta_k \geq \alpha_{k + 1}
    \end{align*}
\end{theorem}

\subsection{The Laplacian and Graph Drawing}

Vectors on graphs are functions $V \to \mathbb{R}$, the vector $\vec{1}$ denotes the function $\vec{1}(a) = 1$.
The degree function $\vec{d}$ is also a vector.

Matrices on graphs are functions $V \times V \to \mathbb{R}$ or can be viewed as linear operators on the space of vectors on graphs.
Let $G$ be a graph, then we use $M_G$ to denote the adjacency matrix,
$D_G$ to denote the diagonal matrix of vertex degrees,
and $L_G = D_G - M_G$ to denote the Laplacian matrix.
Observe that $M_G \vec{1} = \vec{d}$ and $L_G \vec{1} = \vec{0}$.

The Laplacian is also a natural quadratic form on a graph
\begin{align*}
    \vec{x}^T L_G \vec{x} = \sum_{(a, b) \in E(G)} w_{a, b} (\vec{x}(a) - \vec{x}(b))^2
\end{align*}
Let $\lambda_1 = 0 \leq \lambda_2 \leq ... \leq \lambda_n$ be the eigenvalues of $L_G$ with eigenvectors $\vec{\psi}_1, ..., \vec{\psi}_n$.

\begin{lemma}
    $G$ is connected iff $\lambda_2 \neq 0$.
\end{lemma}

We draw a graph in $k$ dimensions by using the eigenvectors corresponding to $\lambda_2, ..., \lambda_{k+1}$ as the coordinates of vertices.
These coordinates minimize the expression (excluding coordinates that lead to trivial drawings):
\begin{align*}
    \sum_{(a, b) \in G} w_{a, b} \norm{x(a) - x(b)}^2 = \sum_{i=1}^{k} \vec{x_i}^T L_G \vec{x_i}
\end{align*}
where $x: V \to \mathbb{R}^k, x = (\vec{x_1}, ..., \vec{x_k})$ is the coordinate function.

\begin{theorem}[Hall's Drawing Theorem]
    Let $\vec{x_1}, ..., \vec{x_k}$ be orthonormal vectors that are orthogonal to $\vec{1}$, then
    \begin{align*}
        \sum_{i=1}^{k} \vec{x_i}^T L_G \vec{x_i} \geq \sum_{i=2}^{k + 1} \lambda_1
    \end{align*}
    where equality holds for $\vec{\psi}_j^T \vec{x_i} = 0$ for all $i$ and $j > k + 1$.
\end{theorem}

\subsection{Adjacency Matrices}

Let the eigenvalues of $M_G$ be $\mu_1 \geq ... \geq \mu_n$.

\begin{lemma}
    Let $d_{avg}$ and $d_{max}$ be the average and maximum degrees respectively, then
    \begin{align*}
        d_{avg} \leq \mu_1 \leq d_{max}
    \end{align*}
    further, if $H$ be a subgraph of $G$, then
    \begin{align*}
        d_{avg}(H) \leq \mu_1
    \end{align*}
\end{lemma}

\begin{lemma}
    If $G$ is connected and $\mu_1 = d_{max}$ then $G$ is $d_{max}$-regular.
\end{lemma}

\begin{theorem}[Wilf's Theorem]
    Let $\chi(G)$ be the chromatic number of $G$, then
    \begin{align*}
        \chi(G) \leq \lfloor\mu_1\rfloor + 1
    \end{align*}
\end{theorem}

\begin{lemma}
    Let $G$ be a connected weighted graph and $\vec{\phi}$ be a non negative eigenvector of $M_G$, then $\vec{\phi}$ is strictly positive.
\end{lemma}

\begin{theorem}[Perron-Frobenius Theorem]
    Let $G$ be connected, then
    \begin{enumerate}
        \item $\mu_1$ has a strictly positive eigenvector
        \item $\mu_1 \geq - \mu_n$
        \item $\mu_1 > \mu_2$
    \end{enumerate}
\end{theorem}

\begin{lemma}
    If $G$ is bipartite, then the eigenvalues of $M_G$ are symmetric about $0$.
\end{lemma}

\begin{theorem}
    Let $G$ be connected, $\mu_1 = - \mu_n$ iff $G$ is bipartite.
\end{theorem}

\subsection{Comparing Graphs}

We introduce the partial order $\succeq$ on matrices as
\begin{align*}
    A \succeq B \iff \forall \vec{x}, \vec{x}^T A \vec{x} \geq \vec{x}^T B \vec{x}
\end{align*}
In particular $A \succeq 0$ means $A$ is positive semidefinite.
For graphs $G, H$ on the same set of vertices, we write $G \succeq H$ iff $L_G \succeq L_H$.
If $H$ is a subset of $G$, we have
\begin{align*}
    G \succeq H
\end{align*}

For a graph $H$, define $c \cdot H$ to be the graph $H$ with each edge weight multiplied by $c$.
Let $\lambda_k(H)$ denote the $k$th smallest eigenvalue of $L_H$.
\begin{lemma}
    If $G, H$ are graphs
    \begin{align*}
        G \succeq c \cdot H \implies \lambda_k(G) \geq c \lambda_k(H)
    \end{align*}
\end{lemma}

Let $G_{a, b}$ be the graph with only the edge $(a, b)$.
The proof of the following lemmas follow trivially from the Cauchy Schwarz inequality applied to the Laplacian.

\begin{lemma}
    Let $P_n$ be the path graph on $n$ vertices between vertex $1$ and $n$.
    \begin{align*}
        G_{1, n} \preceq (n - 1)P_n
    \end{align*}
\end{lemma}

\begin{lemma}[Extension to Weighted Paths]
    Let $P_{n, w}$ be the weighted path graph on $n$ vertices with $w_i$ the weight on the edge $(i, i+1)$.
    \begin{align*}
        G_{1, n} \preceq \left(\sum_{i=1}^{n-1} \frac{1}{w_i}\right) P_{n, w}
    \end{align*}
\end{lemma}

We use these lemmas to prove bounds on the eigenvalues of graphs.

Let $K_n$ be the complete graph on $n$ vertices, it is easy to see that $\lambda_i(K_n) = n$ for $i \geq 2$.
We can also write that
\begin{align*}
    L_{K_n} = \sum_{a < b} L_{G_{a, b}}
\end{align*}
this allows us to prove the following
\begin{gather*}
    K_n = \sum_{a < b} G_{a, b} \preceq \sum_{a < b} (b - a) P_{a, b} \preceq \sum_{a < b} (b - a) P_n \\
    \implies \lambda_2(K_n) = n \leq \lambda_2(P_n) \sum_{a < b} (b - a) = \frac{n (n + 1) (n - 1)}{6} \lambda_2(P_n)
\end{gather*}

\begin{lemma}[Bounding $\lambda_2$ of the Path Graph]
    \begin{align*}
        \lambda_2(P_n) \geq \frac{6}{(n+1)(n-1)}
    \end{align*}
\end{lemma}

Let $T_d$ be the complete binary tree of depth $d$, this will have $n = 2^{d+1} - 1$ vertices.
Let $T_d^{a, b}$ be the shortest path between vertices $a, b$ on $T_d$.
Note that this path has size atmost $2d \leq 2 \log_{2} n$. Doing a similar comparision with $K_n$ we get
\begin{gather*}
    K_n = \sum_{a < b} G_{a, b} \preceq \sum_{a < b} 2d T_d^{a, b} \preceq \sum_{a < b} 2 \log_2 n T_d = \binom{n}{2} 2 \log_2 n T_d \\
    \implies \lambda_2(K_n) = n \leq \binom{n}{2} 2 \log_2 n \lambda_2(T_d)
\end{gather*}

\begin{lemma}[Bounding $\lambda_2$ of Complete Binary Trees]
    \begin{align*}
        \lambda_2(T_d) \geq \frac{1}{(n - 1) \log_2 n}
    \end{align*}
\end{lemma}

\section{Eigenvalues and Eigevectors of Some Graphs}

\subsection{Quotient Graphs}

\begin{definition}[Equitable Partition]
    Given a graph $G$, let $C = (C_1, C_2, ..., C_k)$ be a partition of $V(G)$, the elements of the partition are called cells.
    Such a partition is said to equitable if the number of edges between a fixed vertex in $C_i$ and $C_j$
    is independent of that fixed vertex, we denote this number by $c_{ij}$.        
\end{definition}

\begin{lemma}
    A partition is equitable iff the induced subgraph of each cell is regular and the edges between two cells form a biregular graph.
\end{lemma}

As a particular example, the orbits in the action of the automorphism group of a graph form an equitable partition.

\begin{definition}[Quotient Graph]
    Given a partition $C$ of $G$, the quotient $G/C$ is defined to be the graph with vertices $C$ and a cell $C_i$ has a directed edge to $C_j$ of weight $c_{ij}$.
\end{definition}

In general the quotient graph has loops, non unit weights and cannot be represented as an undirect graph ($c_{ij} \neq c_{ji}$).
We will show that the characteristic polynomial of $M_{G/C}$ divides that of $M_{G}$.

If $|G| = n, |C| = k$, the characteristic matrix $P(C)$ is a $(n,k)$ matrix whose $(i, j)$ entry is whether the $i$th vertex of $G$ is contained in $C_j$.
this gives us a computational way to represent $C$.

\begin{lemma}
    If $C$ is an equitable partition of $G$, then $M_{G}P(C) = P(C)M_{G/C}$. Conversely, if there exists a $B$ such that $M_G P(C) = P(C) B$, then $C$ is equitable.
\end{lemma}

This lemma can be rephrased as: $C$ is equitable iff the column space of $P(C)$ is an invariant subspace of $M_G$.
The column space of $P(C)$ is the vector space of functions on $G$ that are constant on the cells of $C$.

\begin{corollary}
    If $C$ is equitable, then $L_{G}P(C) = P(C)L_{G/C}$. Conversely, if there exists a $B$ such that $L_G P(C) = P(C) B$, then $C$ is equitable.
\end{corollary}

The Laplacian of a graph is invariant under addition and removal of loops (this is not true for the adjacency matrix).
Therefore, the above corollary is also true for $G/C$ replaced by the same graph with its loops removed.

\begin{theorem}
    Let $C$ be an equitable partition of $G$, then for $A \in \set{M, L}$
    \begin{enumerate}
        \item $A_{G/C}\vec{x} = \lambda \vec{x} \implies A_GP\vec{x} = \lambda P \vec{x}$
        \item $A_G \vec{y} = \lambda \vec{y} \implies \vec{y}^T P A_{G/C} = \lambda \vec{y}^T P$
        \item The characteristic polynomial of $A_{G/C}$ divides the characteristic polynomial of $A$.
    \end{enumerate}
\end{theorem}

When $A_{G/C}$ is symmetric, the second condition can be rewritten as
\begin{align*}
    A_G \vec{y} = \lambda y \implies A_{G/C} P^T \vec{y} = \lambda P^T \vec{y}
\end{align*}

\subsection{Cycle Graph}

Represent the vertices of $C_n$ as $\set{0, 1, ..., n - 1}$.
\begin{theorem}
    The cycle graph $C_n$ has eigenvectors
    \begin{align*}
        \vec{x}_k (u) &= \cos(\frac{2\pi k u}{n}) \\
        \vec{y}_k (u) &= \sin(\frac{2\pi k u}{n})
    \end{align*}
    for $0 \leq k \leq n/2$, ignoring $\vec{y}_0$ which is $\vec{0}$,
    with eigenvalue $2 - 2 \cos \frac{2\pi k}{n}$.
\end{theorem}

\subsection{Path Graph}

Consider $C_{2n}$ and the partition $\sigma$ defined as $(\set{0, 2n - 1}, \set{1, 2n - 2}, ... \set{n - 1, n})$.
Clearly this partition is equitable and $C_{2n}/\sigma$ is $P_n$ with loops at the endpoint vertices.
From the previous section, we can write
\begin{align*}
    L_{C_{2n}} P = P L_{P_n}
\end{align*}
Since $L_{P_n}$ is symmetric, the the following must be eigenvectors of $P_n$
\begin{align*}
    P^T \vec{x}_k(u) &= \cos(\frac{\pi k u}{n}) + \cos(\frac{\pi k (2n - 1 - u)}{n}) \\
    &= \cos(\frac{\pi k u}{n}) + \cos(\frac{\pi k (u + 1)}{n}) \\
    &= 2 \cos(\frac{\pi k}{2n}) \cos(\frac{\pi k (u + \frac{1}{2})}{n}) \\
    P^T \vec{y}_k(u) &= \sin(\frac{\pi k u}{n}) + \sin(\frac{\pi k (2n - 1 - u)}{n}) \\
    &= \sin(\frac{\pi k u}{n}) - \sin(\frac{\pi k (u + 1)}{n}) \\
    &= - 2 \sin(\frac{\pi k}{2n}) \cos(\frac{\pi k (u + \frac{1}{2})}{n})
\end{align*}
Hence the eigenvector corresponding to eigenvalue $2 - 2 \cos\frac{\pi k}{n}$ is
\begin{align*}
    \vec{\phi}_k (u) = \cos(\frac{\pi k (u + \frac{1}{2})}{n})
\end{align*}

\section{Random Graphs}

\subsection{Introduction}

An Erd\"{o}s-R\'{e}nyi random graph is a graph in which each edge is present with probability $p$, independent of other edges.
We discuss the eigenvalues of the adjacency matrix.
The largest eigenvalue of such a graph is close to $pn$ and all other eigenvalues are usually atmost of order $\sqrt{np}$.
Our goal is to prove formal results regarding the eigenvalues.

We can write the adjacency matrix $M$ of this graph as
\begin{align*}
    M = \expect(M) + R = p(J - I) + R
\end{align*}
where $J$ is the all ones matrix, $I$ is the identity matrix and $R$ is defined for off diagonal lower triangular entries as
\begin{align*}
    R(a, b) = \begin{cases}
                  1 - p & \text{with probability $p$}   \\
                  -p    & \text{with probability $1-p$}
              \end{cases}
\end{align*}
The upper triangular entries are determined from the symmetry of $R$.
Clearly the expectation of $R$ is the zero matrix.
We will show that the eigenvalues of $R$ are usually small
and thus $M$ is approximately $p(J - I)$. Note that the eigenvalues of $p(J - I)$ are $p(n - 1)$ with multiplicity $1$ and $-p$ with multiplicity $n - 1$.

Let the eigenvalues of $R$ be $\rho_1 \geq ... \geq \rho_n$.
Then, the eigenvalues of $R - pI$ are $\rho_i - p$.
Since $pJ$ is rank one, the eigenvalues of $M = R - pI + pJ$ interlace the eigenvalues of $R - pI$ by the \hyperref[thm:cauchy-rank-one]{Cauchy interlacing theorem}.
Hence, the eigenvalues of $R$ can give bounds on the eigenvalues of $M$.

\subsection{Moments of Eigenvalues}

We can get some quantitative results on the eigenvalues of $R$ by calculating the moments of $\rho_i$.
It is easy to verify that the $k$th moment is
\begin{align*}
    \sum_{i=1}^{n} \rho_i^k = \trace(R^k)
\end{align*}
We know that the eigenvalues of $R^k$ are $\rho_i^k$.
From the value of the trace, we know that for even $k$
\begin{align*}
    \rho_1^k \leq Tr(R^k) \implies |\rho_1| \leq Tr(R^k)^{1/k}
\end{align*}
If we calculate a bound: $\expect(Tr(R^k)) < u$, we can write using \hyperref[thm:markov]{Markov's inequality}
\begin{align*}
    \prob(|\rho_1| > (1 + \epsilon)u^{1 / k}) & = \prob(\rho_1^k > (1 + \epsilon)^k u)                  \\
                                              & \leq \prob(Tr(R^l) > (1 + \epsilon)^k \expect(Tr(R^k))) \\
                                              & \leq (1 + \epsilon)^{-k}
\end{align*}
The probability on the RHS will be small if $\epsilon > 1 / k$.
This will in particular be useful if for large $k$ we can find small $u$.

\subsection{Expectation of The Trace}

Let $M$ be a matrix, then we can write
\begin{align*}
    M^l(a, b) = \sum_{v_1, ..., v_{k - 1} \in V} M(a, v_1) M(v_1, v_2) ... M(v_{l - 1}, b)
\end{align*}
Therefore, we can write
\begin{align*}
    \expect(R^l(a_0, a_0)) = \sum_{a_1, ..., a_{l-1} \in V} \expect(R(a_0, a_1) R(a_1, a_2) ... R(a_l, a_0))
\end{align*}
Note that distinct elements of $R$ are independent, therefore the only dependence comes from terms that appear multiple times.
Let $S$ be the multiset $\set{\set{a_0, a_1}, \set{a_1, a_2}, ..., \set{a_{l-1}, a_0}}$ and $S'$ denote the set of the same elements, then
\begin{align*}
    \expect(R(a_0, a_1) R(a_1, a_2) ... R(a_l, a_0)) = \prod_{\set{b_i, c_i} \in S'} \expect(R(b_i, c_i)^{d_i})
\end{align*}
where $d_i$ denotes the number of $\set{b_i, c_i}$ in $S$.
The RHS is clearly zero if any $d_i$ is $1$. For $d \geq 2$, we can write
\begin{align*}
    \expect(R(b_i, c_i)^d) & = (1 - p)^d p + (-p)^d (1 - p) = p(1 - p) \left((1-p)^{d - 1} - (-p)^{d-1}\right) \leq p(1 - p)
\end{align*}
this gives us
\begin{align*}
    \expect(R(a_0, a_1) R(a_1, a_2) ... R(a_l, a_0)) \leq (p(1-p))^{|S'|}
\end{align*}

We define a closed walk of length $l$ as a sequence of vertices $a_0, a_1, ..., a_{l - 1}, a_l$ such that $a_l = a_0$.
A closed walk is called significant if the multiset $\set{\set{a_0, a_1}, \set{a_1, a_2}, ... \set{a_{l-1}, a_l}}$ has atleast two copies of each element.

\begin{definition}
    Let $W_{n, l, k}$ denote the number of significant closed walks of length $l$ in a graph with $n$ vertices such that the above multiset has $k$ distinct elements.
\end{definition}

\begin{lemma}
    \begin{align*}
        \expect(\trace(R^l)) \leq \sum_{k = 1}^{l / 2} W_{n, l, k} (p(1-p))^k
    \end{align*}
\end{lemma}

\subsection{The Number of Walks}

\begin{lemma}
    \begin{align*}
        W_{n, l, k} \leq n^{k + 1}2^ll^{4(l - 2k)}
    \end{align*}
\end{lemma}

\subsection{Putting It All Together}

Using the bound on the number of walks, we can write
\begin{align*}
    \expect(\trace(R^l)) & \leq \sum_{k=1}^{l/2} n^{k + 1}2^ll^{4(l - 2k)} (p(1-p))^k  \\
                         & = n 2^l l^{4l} \sum_{k=1}^{l/2} (np(1-p)l^{-8})^k           \\
                         & = n (4np(1-p))^{l/2} \sum_{k=1}^{l/2} (np(1-p)l^{-8})^{1-k}
\end{align*}
for $l^8 \leq np(1-p)$, we get
\begin{align*}
    \expect(\trace(R^l)) & \leq n (4np(1-p))^{l/2} \sum_{k=1}^{l/2} 2^{1-k} \\
                         & \leq 2n(4np(1-p))^{l/2}
\end{align*}
Substituting back in the markov identity, we get
\begin{align*}
    \prob(|\rho_1| \geq (1 + \epsilon) (2n)^{1/l}(4np(1-p))^{1/2}) \leq (1 + \epsilon)^{-l}
\end{align*}
Note that $(2n)^{1/l}$ can be written as
\begin{align*}
    (2n)^{1/l} = 2^{\log_2{2n}/l}
\end{align*}
We get the required result, that the eigenvalues are of order $\sqrt{np}$, if $l$ satisfies
\begin{align*}
    1 + \log_2{n} < l < (np(1-p))^{1/8}
\end{align*}
Such an $l$ can be found for sufficiently large $n$, for $p = 0.5$, $n$ roughly needs to satisfy
\begin{align*}
    n^{1/8} > 1 + \log_2 n
\end{align*}
This is not satisfied for $n = 2^{40} \approx 10^{12}$.
Therefore while asymptotically useful, this result may not be practically useful.

\section{Random Walks on Graphs}

A random walk on a weighted undirected graph $G$ is a walk that randomly moves to an adjacent vertex with probability proportional
to the weight of the edge. The probability density at time $t + 1$ can be written as
\begin{align*}
    \vec{p}_{t + 1}(a) &= \sum_{b: (a, b) \in E(G)} \frac{w(a, b)}{\vec{d}(b)} \vec{p}_t(b) \\
    \implies \vec{p}_{t + 1} &= M D^{-1} \vec{p}_t
\end{align*}
The walk matrix $W$ is defined as $MD^{-1}$. The lazy walk matrix is defined as
\begin{align*}
    \widetilde{W} = \frac{1}{2} I + \frac{1}{2} W
\end{align*}
The lazy walk remains at the current vertex with probability half, this is more well behaved than the walk matrix
(in terms of convergence of $\vec{p}_t$).

\subsection{Convergence}

The walk matrix is similar to the normalized adjacency matrix which is symmetric and thus has nice properties of eigenvectors
\begin{align*}
    \widetilde{M} = D^{-\frac{1}{2}} W D^{\frac{1}{2}} = D^{-\frac{1}{2}} M D^{-\frac{1}{2}}
\end{align*}

\begin{lemma}
    $\vec{\psi}$ is an eigenvector of $\widetilde{M}$ iff $D^{\frac{1}{2}}\vec{\psi}$ is an eigenvector of $W$, further they have same eigenvalue
    and the eigenvectors of $W$ are the same as $\widetilde{W}$.
\end{lemma}

Note that $\vec{d}$ is a positive eigenvector of $W$ with eigenvalue $1$.
By Perron-Frobenius theorem, the eigenvalues of $W$ must lie between $-1$ and $1$.
The eigenvalues of $\widetilde{W}$ are simple of the form $(1 + \lambda_i)/2$ where $\lambda_i$ are eigenvectors of $W$.
Hence, the eigenvalue of $\widetilde{W}$ lie between $0$ and $1$.

Clearly for a probability distribution to be stable with respect to the lazy random walk, it must be an eigenvector of $\widetilde{W}$ of eigenvalue $1$.
This distribution is
\begin{align*}
    \vec{\pi} = \frac{\vec{d}}{\vec{1}^T\vec{d}}
\end{align*}
Now we will prove that the probability distribution of the lazy random walk converges to $\vec{\pi}$.
Let $\vec{\psi}_i$ be the orthonormal eigenvectors of $\widetilde{M}$
corresponding to eigenvalues $\omega_i$ of $\widetilde{W}$ ($1 = \omega_1 > \omega_2 \geq ... \geq 0$).
The eigenvalues of $\widetilde{M}$ are then $2 \omega_i - 1$.
Note that
\begin{align*}
    \vec{\psi}_1 \propto D^{-\frac{1}{2}} \vec{d} = \sqrt{\vec{d}} \implies \vec{\psi}_1 = \frac{\sqrt{\vec{d}}}{\lVert\sqrt{\vec{d}} \rVert }
\end{align*}
and we can write $\vec{p}_0$ as
\begin{align*}
    D^{-\frac{1}{2}}\vec{p}_0 = \sum_{i} c_i \vec{\psi}_i
\end{align*}
and in particular $c_1$ is
\begin{align*}
    c_1 = \vec{\psi}_1^T D^{-\frac{1}{2}}\vec{p}_0 = \frac{\sqrt{\vec{d}}^T}{\lVert\sqrt{\vec{d}}\rVert} (D^{-\frac{1}{2}} \vec{p}_0) = \frac{\vec{1}^T \vec{p}_0}{\lVert\sqrt{\vec{d}}\rVert} = \frac{1}{\lVert\sqrt{\vec{d}}\rVert}
\end{align*}
Now, we can write $\vec{p}_t$ as
\begin{align*}
    \vec{p}_t &= \widetilde{W}^t \vec{p}_0 \\
    &=D^{\frac{1}{2}} \left(D^{-\frac{1}{2}} \widetilde{W} D^{\frac{1}{2}}\right)^t D^{-\frac{1}{2}} \vec{p}_0 \\
    &=D^{\frac{1}{2}} \left(\frac{1}{2} I + \frac{1}{2} \widetilde{M}\right)^t \sum_{i} c_i \vec{\psi}_i \\
    &=D^{\frac{1}{2}} \sum_{i} c_i \omega_i^t \vec{\psi}_i \\
    &=D^{\frac{1}{2}} c_1 \vec{\psi}_1 + D^{\frac{1}{2}} \sum_{i \geq 2} c_i \omega_i^t \vec{\psi}_i
\end{align*}
Note that for sufficiently large $t$, the second term goes to zero as $\omega_i < 1$ for $i \geq 2$.
Evaluating the first term, we get
\begin{align*}
    \lim_{t \to \infty} \vec{p}_t = D^{\frac{1}{2}} c_1 \vec{\psi}_1 = D^{\frac{1}{2}} \frac{\sqrt{\vec{d}}}{\lVert \sqrt{\vec{d}} \rVert^2} = \frac{\vec{d}}{\vec{1}^T \vec{d}} = \vec{\pi}
\end{align*}

\subsection{Rate of Convergence}

We measure the rate of convergence pointwise, by calculating upper bounds on $|\vec{p}_t(b) - \vec{\pi}(b)|$.
Assuming that $\vec{p}_0 = \vec{\delta}_a$
\begin{align*}
    \vec{p}_t &= \vec{\pi} + D^{\frac{1}{2}} \sum_{i \geq 2} c_i \omega_i^t \vec{\psi}_i \\
    \implies \vec{p}_t(b) - \vec{\pi}(b) &= \vec{\delta}_b^T D^{\frac{1}{2}} \sum_{i \geq 2}\left(\vec{\psi}_i^T D^{-\frac{1}{2}} \vec{\delta}_a\right) \omega_i^t \vec{\psi}_i \\
    &= \sqrt{\vec{d}(b)} \vec{\delta}_b^T \sum_{i \geq 2}\frac{(\vec{\psi}_i^T \vec{\delta}_a)}{\sqrt{d(a)}} \omega_i^t \vec{\psi}_i \\
    &= \sqrt{\frac{\vec{d}(b)}{\vec{d}(a)}} \sum_{i \geq 2}\omega_i^t(\vec{\psi}_i^T \vec{\delta}_a) (\vec{\delta}_b^T \vec{\psi}_i)
\end{align*}
We can bound the term on the right like
\begin{align*}
    \left| \sum_{i \geq 2}\omega_i^t(\vec{\psi}_i^T \vec{\delta}_a) (\vec{\delta}_b^T \vec{\psi}_i) \right| &\leq \sum_{i \geq 2} \omega_i^t |\vec{\psi}_i^T \vec{\delta}_a| |\vec{\delta}_b^T \vec{\psi}_i| \\
    &\leq \omega_2^t \sum_{i \geq 1} |\vec{\psi}_i^T \vec{\delta}_a| |\vec{\delta}_b^T \vec{\psi}_i| \\
    &\leq \omega_2^t \lVert \vec{\delta}_a \rVert \lVert \vec{\delta}_b \rVert \\
    &\leq \omega_2^t
\end{align*}

\begin{theorem}
    Given $p_0 = \vec{\delta}_a$, we can write
    \begin{align*}
        |\vec{p}_t(b) - \vec{\pi}(b)| \leq \sqrt{\frac{\vec{d}(b)}{\vec{d}(a)}} \omega_2^t
    \end{align*}
\end{theorem}

\subsection{Normalized Laplacian}

The normalized Laplacian $\widetilde{L}$ is defined similar to $\widetilde{M}$
\begin{align*}
    \widetilde{L} = D^{-\frac{1}{2}} L D^{-\frac{1}{2}} = I - \widetilde{M}
\end{align*}

\begin{lemma}
    Let $\lambda_1 \leq ... \leq \lambda_n$ be eigenvalues of $L$ and $v_1 \leq ... \leq v_n$ eigenvalues of $\widetilde{L}$.
    Then, we have
    \begin{align*}
        \frac{\lambda_i}{d_{min}} \leq v_i \leq \frac{\lambda_i}{d_{max}}
    \end{align*}
\end{lemma}

The proof follows easily from a change of variables in the Courant Fischer theorem.

It is easy to see that the eigenvalues of $\widetilde{L}$ are of the form $v_i = 2 - 2 \omega_i$.
We say the walk has \textbf{mixed} at time $t$ if $|\vec{p}_t(b) - \vec{\pi}(b)| \leq \vec{\pi}(b)/2$.
We can easily compute a lower bound on $t$ for this to be true using the above theorem on convergence rate
\begin{align*}
    \sqrt{\frac{\vec{d}(b)}{\vec{d}(a)}} \omega_2^t &\leq \frac{\vec{d}(b)}{2 \vec{1}^T \vec{d}} \\
    \iff \omega_2^t &\leq \frac{\sqrt{\vec{d}(a)\vec{d}(b)}}{2 \vec{1}^T \vec{d}} \\
    \iff \left(1 - \frac{v_2}{2}\right)^t &\leq \frac{\sqrt{\vec{d}(a)\vec{d}(b)}}{2 \vec{1}^T \vec{d}} \\
    \implies \exp{-\frac{tv_2}{2}} &\leq \frac{\sqrt{\vec{d}(a)\vec{d}(b)}}{2 \vec{1}^T \vec{d}} \\
    \iff -\frac{tv_2}{2} &\leq \ln{\frac{\sqrt{\vec{d}(a)\vec{d}(b)}}{2 \vec{1}^T \vec{d}}} \\
    \iff t &\geq \frac{2}{v_2} \ln{\frac{2 \vec{1}^T \vec{d}}{\sqrt{\vec{d}(a)\vec{d}(b)}}} = \frac{O(\ln{n})}{v_2} \\
\end{align*}
To get the equivalence in the other direction, we need to find $t'$ such that
\begin{align*}
    \left(1 - \frac{v_2}{2}\right)^{t'} &\leq \exp{-\frac{tv_2}{2}} \\
    \iff t' \ln\left(1 - \frac{v_2}{2}\right) &\leq -\frac{tv_2}{2} \\
    \iff t' &\geq \frac{v_2}{2 \ln{\frac{1}{1 - v_2/2}}} t
\end{align*}
Let the factor on the RHS be $c$, then
\begin{align*}
    t &\geq c \frac{2}{v_2} \ln{\frac{2 \vec{1}^T \vec{d}}{\sqrt{\vec{d}(a)\vec{d}(b)}}} \\
    \iff \exp{-\frac{tv_2}{2c}} &\leq \frac{\sqrt{\vec{d}(a)\vec{d}(b)}}{2 \vec{1}^T \vec{d}} \\
    \implies \left(1 - \frac{v_2}{2}\right)^{t'} &\leq \frac{\sqrt{\vec{d}(a)\vec{d}(b)}}{2 \vec{1}^T \vec{d}},\quad t' \geq c\frac{t}{c} = t \\
    \iff \sqrt{\frac{\vec{d}(b)}{\vec{d}(a)}} \omega_2^{t'} &\leq \frac{\vec{d}(b)}{2 \vec{1}^T \vec{d}} \\
    \implies |\vec{\pi}_{t'}(b) - \vec{\pi}(b)| &\leq \frac{\vec{\pi}(b)}{2}
\end{align*}
Therefore, the number of steps required for mixing is of the order
\begin{align*}
    \frac{c}{v_2}O(\ln{n}) = \frac{O(\ln{n})}{-\ln\left(1 - \frac{v_2}{2}\right)} = \frac{O(\ln{n})}{-\ln{\omega_2}}
\end{align*}

\section{Harmonic Functions on Graphs}

We first impose an additional structure on a weighted graph $G$, a non empty set $B \subset V(G)$ called the boundary.
The graph $G$ is now the tuple $(V, E, w, B)$ and we denote $B$ by $B(G)$ or $B_G$. The interior of the graph is then defined by $S(G) = V_G - B_G$.

\begin{definition}
    A function $f$ is harmonic on a graph $G$ if for all $a \in S(G)$
    \begin{align*}
        f(a) = \frac{1}{d(a)} \sum_{b \sim a} w_{ab}f(b)
    \end{align*}
\end{definition}

We can make an analogy between harmonic functions and spring networks. Consider a network of springs in $\mathbb{R}^k$
with node $a$ at position $f(a)$ and neighboring nodes being connected by a harmonic spring of strength $w_{ab}$.
If we fix the nodes in $B$ at some fixed points, the interior nodes, in equilibrium must have the net force on them $0$, which means
\begin{align*}
    \sum_{b \sim a} w_{ab}(f(a) - f(b)) = 0
\end{align*}
This is equivalent to saying that $f$ is harmonic on the graph.
Hence equilibrium states of spring networks correspond to harmonic functions on graphs.

\subsection{Existence and Uniqueness}

We show that for every graph, there exists a unique harmonic function, given the values of the function at the boundary.

\section{Appendix}

\subsection{Linear Algebraic Results}

\begin{theorem}
    Let $A, B$ be commuting symmetric matrices on a vector space $V$, then they can be simultaneously diagonalized by the same unitary transformation.
\end{theorem}
\begin{proof}
    We will prove by induction on the dimension of $V$ that $A, B$ have the same eigenspaces,
    orthonormal basis of these eigenspaces will then provide the unitary transformation.

    Clearly the statement is true for $\dim V = 1$, let $\dim V = n > 1$ and assume the statement is true for all $V$ such that $\dim V < n$.
    Let $\alpha_1, ..., \alpha_k$ be the distinct eigenvalues of $A$. If $k = 1$, then $A = \lambda_1 I$ and thus we are done.
    So assume $k \geq 2$, let $\Lambda_i$ be the eigenspace corresponding to the eigenvalue $\alpha_i$.
    Then, $\Lambda_i$ is an invariant subspace of $B$ since for any $\vec{v} \in \Lambda_i$
    \begin{align*}
        A(B\vec{v}) = BA\vec{v} = \lambda_i B \vec{v} \implies B\vec{v} \in \Lambda_i
    \end{align*}
    Thus, we can restrict $A, B$ to $\lambda_i$ and apply the induction hypothesis (since $k \geq 2 \implies \dim \Lambda_i < n$)
    to get an eigenbasis of $\Lambda_i$ that simultaneously diagonalizes $A, B$ over $\Lambda_i$. The union of these eigenbases over all $\Lambda_i$ gives us a basis for $V$
    that simultaneously diagonalizes both $A, B$.
\end{proof}

The above theorem is helpful in calculating the eigenvalues of $A + B$.

\begin{theorem}
    Let $M$ be a symmetric matrix with eigenvalues $\lambda_i$, then
    \begin{align*}
        \sum_i \lambda_i = Tr(M)
    \end{align*}
\end{theorem}
\begin{proof}
    Let $D$ be the diagonal matrix of $M$ diagonalized by $U$, then
    \begin{align*}
        M = UDU^T \implies Tr(M) = Tr((UD)U^T) = Tr(U^T (UD)) = Tr(D) = \sum_i \lambda_i
    \end{align*}
\end{proof}

\subsection{Cauchy's Interlacing Theorem for Rank One Updates}

The result that we want to prove is

\begin{theorem}[Cauchy Interlacing Theorem for Rank One Updates]\label{thm:cauchy-rank-one}
    Let $A$ be a symmetric matrix with eigenvalues $\alpha_1 \geq ... \geq \alpha_n$.
    Let $B = A + \vec{x} \vec{x}^T$ for some vector $\vec{x}$, let $B$ have eigenvalues $\beta_1 \geq ... \geq \beta_n$, then
    \begin{align*}
        \beta_i \geq \alpha_i \geq \beta_{i + 1}
    \end{align*}
\end{theorem}

We will do this in two ways. In the first proof, we show that $A$ is a principal submatrix of a matrix with 'similar' eigenvalues to $B$,
this will allow us to apply Cauchy's interlacing theorem for principal submatrices.
In the second proof, we will relate the characteristic polynomials and their roots.

\subsubsection{The First Proof}

We first prove a couple of lemmas that are known as 'Sylvester's theorem'.

\begin{lemma}\label{lemma:sylvester}
    Let $A$ be an $m \times n$ matrix and $B$ be an $n \times m$ matrix, then we can write
    \begin{align*}
        x^n\det(xI_m - AB) = x^m\det(xI_n - BA)
    \end{align*}
\end{lemma}
\begin{proof}
    Define the following matrices
    \begin{align*}
        C = \begin{bmatrix}
                xI_m & A   \\
                B    & I_n
            \end{bmatrix},\quad
        D = \begin{bmatrix}
                I_m & 0    \\
                -B  & xI_n
            \end{bmatrix}
    \end{align*}
    we can see that
    \begin{align*}
        CD = \begin{bmatrix}
                 xI_m - AB & xA   \\
                 0         & xI_n
             \end{bmatrix},\quad
        DC = \begin{bmatrix}
                 xI_m & A         \\
                 0    & xI_n - BA
             \end{bmatrix}
    \end{align*}
    using $\det(CD) = \det(DC)$, we get the required result.
\end{proof}

\begin{lemma}
    Let $A$ be an $m \times n$ matrix and $B$ be an $n \times m$ matrix, then $AB$ and $BA$ have the same set of non zero eigenvalues.
\end{lemma}
\begin{proof}
    This follows trivially from the previous lemma or from the following proof.
    Let $\vec{x}$ be an eigenvector of $AB$ with eigenvalue $\lambda \neq 0$, it is simple to check that $\vec{y} = B\vec{x}$ is an eigenvector of $BA$ with eigenvalue $\lambda$.
\end{proof}

\begin{lemma}
    For positive definite symmetric matrices $A$, a matrix $\sqrt{A}$ such that $\sqrt{A}^2 = A$ exists.
\end{lemma}
\begin{proof}
    Existence follows since the diagonalized matrix of $A$ has a square root.
\end{proof}

We are now ready to prove the theorem.

\begin{proof}
    We will prove the result for positive definite symmetric matrices $A$ by adding $(1 - \alpha_n) I$ to $A$.

    Let $\sqrt{A}$ be a square root of $A$, now define the $n \times (n + 1)$ matrix $C$ as
    \begin{align*}
        C := \begin{bmatrix}
                 \sqrt{A} & \vec{x}
             \end{bmatrix}
    \end{align*}
    then we can write
    \begin{align*}
        CC^T = B, \quad C^T C = \begin{bmatrix}
                                    A            & \sqrt{A}x \\
                                    x^T \sqrt{A} & x^T x
                                \end{bmatrix}
    \end{align*}
    By previous lemma, $C^TC$ and $CC^T$ share the same set of nonnegative eigenvalues. The eigenvalues of $A$ interlace the eigenvalues of $C^T C$ since it is a principal submatrix.
    The eigenvalues of $C^TC$ are thus all positive except the smallest (which is $0$). Since the eigenvalues of $CC^T = B$ are positive, the result follows.
\end{proof}

\subsubsection{The Second Proof}

\begin{definition}[Cyclic Vectors]
    Let $A$ be a linear operator on a vector space $V$.
    $\vec{x} \in V$ is called cyclic if the set of finite linear combinations of $\setc{A^n\vec{x}}{n \in \mathbb{N}_0}$ equals $V$.
\end{definition}

The above definition is equivalent to saying that $\vec{x} \in V$ is cyclic if
\begin{align*}
    V = \setc{p(A)\vec{x}}{p \in \mathbb{R}[t]}
\end{align*}

\begin{lemma}
    A symmetric linear operator $A$ on a vector space $V$ has a cyclic vector iff $A$ has no repeated eigenvalues.
\end{lemma}
\begin{proof}
    Suppose that $\vec{x}$ is a cyclic vector, then
    \begin{align*}
        V = \setc{p(A)\vec{x}}{p \in \mathbb{R}[t]}
    \end{align*}
    Fix an orthonormal basis on $V$, since $A$ is symmetric, it is diagonalizable: $A = UDU^T$ where $D$ is diagonal and $U$ is orthogonal.
    We can then rewrite the above condition as
    \begin{align*}
        V = \setc{p(D)\vec{y}}{p \in \mathbb{R}[t]}, \quad (\vec{y} = U^T\vec{x})
    \end{align*}
    Note that $\setc{p(D)}{p \in R[t]}$ is a vector space with dimension equal to the number of distinct eigenvalues of $A$.
    Therefore, $\setc{p(D)\vec{y}}{p \in \mathbb{R}[t]}$ has the same dimension and we cannot have repeated eigenvalues.

    Conversely, suppose $A$ has no repeated eigenvalues, then $\vec{x} = U \vec{y}$ corresponding to $\vec{y} = \vec{1}$ is a cyclic vector
    (it is simple to show that each eigenvector can be written in the form $p(D)\vec{y}$ for some $p$, and thus $V = \setc{p(D)\vec{y}}{p \in \mathbb{R}[t]}$).
\end{proof}

\begin{definition}
    Let $A$ be a matrix, then $\chi(A)$ represents the characteristic polynomial of $A$ given by
    \begin{align*}
        \chi(A)(z) = \det(zI - A)
    \end{align*}
\end{definition}

\begin{lemma}
    Let $A$ be a symmetric linear operator on a vector space $V$, let $W$ be a subspace of $V$.
    Then the eigenvalues of $A$ are the same as the union of the eigenvalues of $A$ restricted to $W$ and $W^\perp$.
\end{lemma}

We are now ready to prove the theorem.

\begin{proof}
    Define $W := \setc{p(A)\vec{x}}{p \in \mathbb{R}[t]}$ and let $W^\perp$ denote the orthogonal vector space.
    Since $\vec{x} \in W$, $\vec{x}^T \vec{v} = 0$ for all $\vec{v} \in W^\perp$, this means that $A = B$ on $W^\perp$.
    Now, let us retrict $A, B$ to $W$, let $k = \dim W$.
    Since $W$ is cyclic by definition, $A$ has $k$ distinct eigenvalues $\lambda_1 > ... > \lambda_k$ with eigenvectors $\vec{\phi}_i$.
    Let $z$ not be an eigenvalue of $A$, then
    \begin{align*}
        \chi(B)(z) & = \det(zI - A - \vec{x}\vec{x}^T)                      \\
                   & = \det(zI - A) \det(I - (zI - A)^{-1}\vec{x}\vec{x}^T)
    \end{align*}
    $zI-A$ is invertible since $z$ is not an eigenvalue of $A$. We can now use \hyperref[lemma:sylvester]{Sylvester's theorem} with $x = 1$ to write
    \begin{align*}
        \chi(B)(z) & = \chi(A)(z)(1 - \vec{x}^T (zI - A)^{-1} \vec{x})                                              \\
                   & = \chi(A)(z)\left(1 - \sum_{i = 1}^k \frac{(\vec{x}^T \vec{\phi}_i^T)^2}{z - \lambda_i}\right)
    \end{align*}
    Define the function
    \begin{align*}
        G(z) = \sum_{i=1}^k \frac{(\vec{x}^T \vec{\phi}_i)^2}{z - \lambda_i}
    \end{align*}
    Note the following properties of $G$
    \begin{enumerate}
        \item Solutions to $G(z) = 1$ are eigenvalues of $B$.
        \item Since $\phi_i \in W$, we must have $\vec{x}^T \vec{\phi}_i \neq 0$.
        \item There is exactly one solution ($\mu_{i + 1}$) to $G(z) = 1$ for $z \in (\lambda_{i + 1}, \lambda_{i})$, $1 \leq i \leq k - 1$.
        \item There is exactly one solution ($\mu_1$) to $G(z) = 1$ for $z > \lambda_1$.
    \end{enumerate}
    It is now obvious that $\mu_i$ are exactly the eigenvalues of $B$ and they interlace the eigenvalues of $A$.
    \begin{align*}
        \mu_i \geq \lambda_i \geq \mu_{i + 1}
    \end{align*}
    Since $A=B$ on $W^\perp$, the interlacing property is not affected by including the eigenvalues from this vector space.
\end{proof}

\subsection{Probability}

\begin{theorem}[Markov's Inequality]\label{thm:markov}
    Let $X$ be a non negative random variable with $a > 0$, then
    \begin{align*}
        P(X > a E(X)) \leq a^{-1}
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{align*}
        E(X) & = P(X \leq a E(X)) E(X | X \leq a E(X)) + P(X > a E(X)) E(X | X > a E(X))                  \\
             & \implies P(X > a E(X)) \leq \frac{E(X)}{E(X | X > aE(X))} \leq \frac{E(X)}{aE(X)} = a^{-1}
    \end{align*}
\end{proof}

\end{document}
